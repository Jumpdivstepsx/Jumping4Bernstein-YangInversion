#include "macros_neon.h"
#include "polymul_neon.h"
#include <arm_neon.h>

//(x^8-1) = twiddle permute butterfly
int16x8_t twiddle_p_128 = {
    72,    -3430,     3219,     3590,      363,     2020,      591,    -1835
};
//(x^8+1)
int16x8_t twiddle_n_128 = {
    2232,     -864,     681,     -788,    -2047,     1336,      -19,    -2325
};
//(x^8-1)
int16x8_t itwiddle_p_128 = {
    -1837,      589,     2018,     361,     3588,     3217,    -3432,       70
};
//(x^8+1)
int16x8_t itwiddle_n_128 = {
    -2325,      -19,     1336,    -2047,     -788,      681,     -864,     2232
};

//twist_table: 0-31; nR_overq2: 32-63
int16x8_t twist128[64] = {
    {4051, 143, 1654, 4170, 171, 3551, 2996, 975}, {2930, 3390, 2418, 839, 2898, 1222, 2448, 576}, {143, 4170, 3551, 975, 3390, 839, 1222, 576}, {4051, 1654, 171, 2996, 2930, 2418, 2898, 2448}, {1654, 3551, 2930, 839, 2448, 4051, 4170, 2996}, {3390, 2898, 576, 143, 171, 975, 2418, 1222}, {4170, 975, 839, 576, 1654, 2996, 2418, 2448}, {143, 3551, 3390, 1222, 4051, 171, 2930, 2898}, {171, 3390, 2448, 1654, 975, 2898, 4051, 3551}, {2418, 576, 4170, 2930, 1222, 143, 2996, 839}, {3551, 839, 4051, 2996, 2898, 143, 975, 1222}, {1654, 2930, 2448, 4170, 3390, 576, 171, 2418}, {2996, 1222, 4170, 2418, 4051, 975, 2448, 171}, {839, 143, 2930, 576, 3551, 2898, 1654, 3390}, {975, 576, 2996, 2448, 3551, 1222, 171, 2898}, {4170, 839, 1654, 2418, 143, 3390, 4051, 2930}, {2930, 4051, 3390, 143, 2418, 1654, 839, 4170}, {2898, 171, 1222, 3551, 2448, 2996, 576, 975}, {3390, 1654, 2898, 3551, 576, 2930, 143, 839}, {171, 2448, 975, 4051, 2418, 4170, 1222, 2996}, {2418, 171, 576, 3390, 4170, 2448, 2930, 1654}, {1222, 975, 143, 2898, 2996, 4051, 839, 3551}, {839, 2996, 143, 1222, 2930, 4170, 576, 2418}, {3551, 4051, 2898, 975, 1654, 2448, 3390, 171}, {2898, 2930, 171, 4051, 1222, 3390, 3551, 143}, {2448, 2418, 2996, 1654, 576, 839, 975, 4170}, {1222, 2418, 975, 171, 143, 576, 2898, 3390}, {2996, 4170, 4051, 2448, 839, 2930, 3551, 1654}, {2448, 2898, 2418, 2930, 2996, 171, 1654, 4051}, {576, 1222, 839, 3390, 975, 3551, 4170, 143}, {576, 2448, 1222, 2898, 839, 2418, 3390, 2930}, {975, 2996, 3551, 171, 4170, 1654, 143, 4051},{28914, 1021, 11805, 29763, 1221, 25345, 21384, 6959}, {20913, 24196, 17258, 5988, 20684, 8722, 17472, 4111}, {1021, 29763, 25345, 6959, 24196, 5988, 8722, 4111}, {28914, 11805, 1221, 21384, 20913, 17258, 20684, 17472}, {11805, 25345, 20913, 5988, 17472, 28914, 29763, 21384}, {24196, 20684, 4111, 1021, 1221, 6959, 17258, 8722}, {29763, 6959, 5988, 4111, 11805, 21384, 17258, 17472}, {1021, 25345, 24196, 8722, 28914, 1221, 20913, 20684}, {1221, 24196, 17472, 11805, 6959, 20684, 28914, 25345}, {17258, 4111, 29763, 20913, 8722, 1021, 21384, 5988}, {25345, 5988, 28914, 21384, 20684, 1021, 6959, 8722}, {11805, 20913, 17472, 29763, 24196, 4111, 1221, 17258}, {21384, 8722, 29763, 17258, 28914, 6959, 17472, 1221}, {5988, 1021, 20913, 4111, 25345, 20684, 11805, 24196}, {6959, 4111, 21384, 17472, 25345, 8722, 1221, 20684}, {29763, 5988, 11805, 17258, 1021, 24196, 28914, 20913}, {20913, 28914, 24196, 1021, 17258, 11805, 5988, 29763}, {20684, 1221, 8722, 25345, 17472, 21384, 4111, 6959}, {24196, 11805, 20684, 25345, 4111, 20913, 1021, 5988}, {1221, 17472, 6959, 28914, 17258, 29763, 8722, 21384}, {17258, 1221, 4111, 24196, 29763, 17472, 20913, 11805}, {8722, 6959, 1021, 20684, 21384, 28914, 5988, 25345}, {5988, 21384, 1021, 8722, 20913, 29763, 4111, 17258}, {25345, 28914, 20684, 6959, 11805, 17472, 24196, 1221}, {20684, 20913, 1221, 28914, 8722, 24196, 25345, 1021}, {17472, 17258, 21384, 11805, 4111, 5988, 6959, 29763}, {8722, 17258, 6959, 1221, 1021, 4111, 20684, 24196}, {21384, 29763, 28914, 17472, 5988, 20913, 25345, 11805}, {17472, 20684, 17258, 20913, 21384, 1221, 11805, 28914}, {4111, 8722, 5988, 24196, 6959, 25345, 29763, 1021}, {4111, 17472, 8722, 20684, 5988, 17258, 24196, 20913}, {6959, 21384, 25345, 1221, 29763, 11805, 1021, 28914}
};
int16x8_t twist128_inv[64] = {
    {287, 72, 306, 3596, 1510, 4122, 1450, 3867}, {1514, 4139, 2670, 4461, 1743, 1669, 3650, 4035}, {72, 3596, 4122, 3867, 4139, 4461, 1669, 4035}, {287, 306, 1510, 1450, 1514, 2670, 1743, 3650}, {306, 4122, 1514, 4461, 3650, 287, 3596, 1450}, {4139, 1743, 4035, 72, 1510, 3867, 2670, 1669}, {3596, 3867, 4461, 4035, 306, 1450, 2670, 3650}, {72, 4122, 4139, 1669, 287, 1510, 1514, 1743}, {1510, 4139, 3650, 306, 3867, 1743, 287, 4122}, {2670, 4035, 3596, 1514, 1669, 72, 1450, 4461}, {4122, 4461, 287, 1450, 1743, 72, 3867, 1669}, {306, 1514, 3650, 3596, 4139, 4035, 1510, 2670}, {1450, 1669, 3596, 2670, 287, 3867, 3650, 1510}, {4461, 72, 1514, 4035, 4122, 1743, 306, 4139}, {3867, 4035, 1450, 3650, 4122, 1669, 1510, 1743}, {3596, 4461, 306, 2670, 72, 4139, 287, 1514}, {1514, 287, 4139, 72, 2670, 306, 4461, 3596}, {1743, 1510, 1669, 4122, 3650, 1450, 4035, 3867}, {4139, 306, 1743, 4122, 4035, 1514, 72, 4461}, {1510, 3650, 3867, 287, 2670, 3596, 1669, 1450}, {2670, 1510, 4035, 4139, 3596, 3650, 1514, 306}, {1669, 3867, 72, 1743, 1450, 287, 4461, 4122}, {4461, 1450, 72, 1669, 1514, 3596, 4035, 2670}, {4122, 287, 1743, 3867, 306, 3650, 4139, 1510}, {1743, 1514, 1510, 287, 1669, 4139, 4122, 72}, {3650, 2670, 1450, 306, 4035, 4461, 3867, 3596}, {1669, 2670, 3867, 1510, 72, 4035, 1743, 4139}, {1450, 3596, 287, 3650, 4461, 1514, 4122, 306}, {3650, 1743, 2670, 1514, 1450, 1510, 306, 287}, {4035, 1669, 4461, 4139, 3867, 4122, 3596, 72}, {4035, 3650, 1669, 1743, 4461, 2670, 4139, 1514}, {3867, 1450, 4122, 1510, 3596, 306, 72, 287}, {2048, 514, 2184, 25666, 10778, 29421, 10349, 27600}, {10806, 29542, 19057, 31840, 12441, 11912, 26052, 28800}, {514, 25666, 29421, 27600, 29542, 31840, 11912, 28800}, {2048, 2184, 10778, 10349, 10806, 19057, 12441, 26052}, {2184, 29421, 10806, 31840, 26052, 2048, 25666, 10349}, {29542, 12441, 28800, 514, 10778, 27600, 19057, 11912}, {25666, 27600, 31840, 28800, 2184, 10349, 19057, 26052}, {514, 29421, 29542, 11912, 2048, 10778, 10806, 12441}, {10778, 29542, 26052, 2184, 27600, 12441, 2048, 29421}, {19057, 28800, 25666, 10806, 11912, 514, 10349, 31840}, {29421, 31840, 2048, 10349, 12441, 514, 27600, 11912}, {2184, 10806, 26052, 25666, 29542, 28800, 10778, 19057}, {10349, 11912, 25666, 19057, 2048, 27600, 26052, 10778}, {31840, 514, 10806, 28800, 29421, 12441, 2184, 29542}, {27600, 28800, 10349, 26052, 29421, 11912, 10778, 12441}, {25666, 31840, 2184, 19057, 514, 29542, 2048, 10806}, {10806, 2048, 29542, 514, 19057, 2184, 31840, 25666}, {12441, 10778, 11912, 29421, 26052, 10349, 28800, 27600}, {29542, 2184, 12441, 29421, 28800, 10806, 514, 31840}, {10778, 26052, 27600, 2048, 19057, 25666, 11912, 10349}, {19057, 10778, 28800, 29542, 25666, 26052, 10806, 2184}, {11912, 27600, 514, 12441, 10349, 2048, 31840, 29421}, {31840, 10349, 514, 11912, 10806, 25666, 28800, 19057}, {29421, 2048, 12441, 27600, 2184, 26052, 29542, 10778}, {12441, 10806, 10778, 2048, 11912, 29542, 29421, 514}, {26052, 19057, 10349, 2184, 28800, 31840, 27600, 25666}, {11912, 19057, 27600, 10778, 514, 28800, 12441, 29542}, {10349, 25666, 2048, 26052, 31840, 10806, 29421, 2184}, {26052, 12441, 19057, 10806, 10349, 10778, 2184, 2048}, {28800, 11912, 31840, 29542, 27600, 29421, 25666, 514}, {28800, 26052, 11912, 12441, 31840, 19057, 29542, 10806}, {27600, 10349, 29421, 10778, 25666, 2184, 514, 2048}
};

int16x8_t twist128_inv_x1[64] = {
    {2228, 287, 72, 306, 3596, 1510, 4122, 1450}, {3867, 1514, 4139, 2670, 4461, 1743, 1669, 3650}, {2228, 72, 3596, 4122, 3867, 4139, 4461, 1669}, {4035, 287, 306, 1510, 1450, 1514, 2670, 1743}, {2228, 306, 4122, 1514, 4461, 3650, 287, 3596}, {1450, 4139, 1743, 4035, 72, 1510, 3867, 2670}, {2228, 3596, 3867, 4461, 4035, 306, 1450, 2670}, {3650, 72, 4122, 4139, 1669, 287, 1510, 1514}, {2228, 1510, 4139, 3650, 306, 3867, 1743, 287}, {4122, 2670, 4035, 3596, 1514, 1669, 72, 1450}, {2228, 4122, 4461, 287, 1450, 1743, 72, 3867}, {1669, 306, 1514, 3650, 3596, 4139, 4035, 1510}, {2228, 1450, 1669, 3596, 2670, 287, 3867, 3650}, {1510, 4461, 72, 1514, 4035, 4122, 1743, 306}, {2228, 3867, 4035, 1450, 3650, 4122, 1669, 1510}, {1743, 3596, 4461, 306, 2670, 72, 4139, 287}, {2228, 1514, 287, 4139, 72, 2670, 306, 4461}, {3596, 1743, 1510, 1669, 4122, 3650, 1450, 4035}, {2228, 4139, 306, 1743, 4122, 4035, 1514, 72}, {4461, 1510, 3650, 3867, 287, 2670, 3596, 1669}, {2228, 2670, 1510, 4035, 4139, 3596, 3650, 1514}, {306, 1669, 3867, 72, 1743, 1450, 287, 4461}, {2228, 4461, 1450, 72, 1669, 1514, 3596, 4035}, {2670, 4122, 287, 1743, 3867, 306, 3650, 4139}, {2228, 1743, 1514, 1510, 287, 1669, 4139, 4122}, {72, 3650, 2670, 1450, 306, 4035, 4461, 3867}, {2228, 1669, 2670, 3867, 1510, 72, 4035, 1743}, {4139, 1450, 3596, 287, 3650, 4461, 1514, 4122}, {2228, 3650, 1743, 2670, 1514, 1450, 1510, 306}, {287, 4035, 1669, 4461, 4139, 3867, 4122, 3596}, {2228, 4035, 3650, 1669, 1743, 4461, 2670, 4139}, {1514, 3867, 1450, 4122, 1510, 3596, 306, 72}, {15902, 2048, 514, 2184, 25666, 10778, 29421, 10349}, {27600, 10806, 29542, 19057, 31840, 12441, 11912, 26052}, {15902, 514, 25666, 29421, 27600, 29542, 31840, 11912}, {28800, 2048, 2184, 10778, 10349, 10806, 19057, 12441}, {15902, 2184, 29421, 10806, 31840, 26052, 2048, 25666}, {10349, 29542, 12441, 28800, 514, 10778, 27600, 19057}, {15902, 25666, 27600, 31840, 28800, 2184, 10349, 19057}, {26052, 514, 29421, 29542, 11912, 2048, 10778, 10806}, {15902, 10778, 29542, 26052, 2184, 27600, 12441, 2048}, {29421, 19057, 28800, 25666, 10806, 11912, 514, 10349}, {15902, 29421, 31840, 2048, 10349, 12441, 514, 27600}, {11912, 2184, 10806, 26052, 25666, 29542, 28800, 10778}, {15902, 10349, 11912, 25666, 19057, 2048, 27600, 26052}, {10778, 31840, 514, 10806, 28800, 29421, 12441, 2184}, {15902, 27600, 28800, 10349, 26052, 29421, 11912, 10778}, {12441, 25666, 31840, 2184, 19057, 514, 29542, 2048}, {15902, 10806, 2048, 29542, 514, 19057, 2184, 31840}, {25666, 12441, 10778, 11912, 29421, 26052, 10349, 28800}, {15902, 29542, 2184, 12441, 29421, 28800, 10806, 514}, {31840, 10778, 26052, 27600, 2048, 19057, 25666, 11912}, {15902, 19057, 10778, 28800, 29542, 25666, 26052, 10806}, {2184, 11912, 27600, 514, 12441, 10349, 2048, 31840}, {15902, 31840, 10349, 514, 11912, 10806, 25666, 28800}, {19057, 29421, 2048, 12441, 27600, 2184, 26052, 29542}, {15902, 12441, 10806, 10778, 2048, 11912, 29542, 29421}, {514, 26052, 19057, 10349, 2184, 28800, 31840, 27600}, {15902, 11912, 19057, 27600, 10778, 514, 28800, 12441}, {29542, 10349, 25666, 2048, 26052, 31840, 10806, 29421}, {15902, 26052, 12441, 19057, 10806, 10349, 10778, 2184}, {2048, 28800, 11912, 31840, 29542, 27600, 29421, 25666}, {15902, 28800, 26052, 11912, 12441, 31840, 19057, 29542}, {10806, 27600, 10349, 29421, 10778, 25666, 2184, 514}
};

int16x8_t twist128_inv_x4[64] = {
    {1669, 3650, 4035, 2228, 287, 72, 306, 3596}, {1510, 4122, 1450, 3867, 1514, 4139, 2670, 4461}, {2670, 1743, 3650, 2228, 72, 3596, 4122, 3867}, {4139, 4461, 1669, 4035, 287, 306, 1510, 1450}, {3867, 2670, 1669, 2228, 306, 4122, 1514, 4461}, {3650, 287, 3596, 1450, 4139, 1743, 4035, 72}, {1510, 1514, 1743, 2228, 3596, 3867, 4461, 4035}, {306, 1450, 2670, 3650, 72, 4122, 4139, 1669}, {72, 1450, 4461, 2228, 1510, 4139, 3650, 306}, {3867, 1743, 287, 4122, 2670, 4035, 3596, 1514}, {4035, 1510, 2670, 2228, 4122, 4461, 287, 1450}, {1743, 72, 3867, 1669, 306, 1514, 3650, 3596}, {1743, 306, 4139, 2228, 1450, 1669, 3596, 2670}, {287, 3867, 3650, 1510, 4461, 72, 1514, 4035}, {4139, 287, 1514, 2228, 3867, 4035, 1450, 3650}, {4122, 1669, 1510, 1743, 3596, 4461, 306, 2670}, {1450, 4035, 3867, 2228, 1514, 287, 4139, 72}, {2670, 306, 4461, 3596, 1743, 1510, 1669, 4122}, {3596, 1669, 1450, 2228, 4139, 306, 1743, 4122}, {4035, 1514, 72, 4461, 1510, 3650, 3867, 287}, {287, 4461, 4122, 2228, 2670, 1510, 4035, 4139}, {3596, 3650, 1514, 306, 1669, 3867, 72, 1743}, {3650, 4139, 1510, 2228, 4461, 1450, 72, 1669}, {1514, 3596, 4035, 2670, 4122, 287, 1743, 3867}, {4461, 3867, 3596, 2228, 1743, 1514, 1510, 287}, {1669, 4139, 4122, 72, 3650, 2670, 1450, 306}, {1514, 4122, 306, 2228, 1669, 2670, 3867, 1510}, {72, 4035, 1743, 4139, 1450, 3596, 287, 3650}, {4122, 3596, 72, 2228, 3650, 1743, 2670, 1514}, {1450, 1510, 306, 287, 4035, 1669, 4461, 4139}, {306, 72, 287, 2228, 4035, 3650, 1669, 1743}, {4461, 2670, 4139, 1514, 3867, 1450, 4122, 1510}, {11912, 26052, 28800, 15902, 2048, 514, 2184, 25666}, {10778, 29421, 10349, 27600, 10806, 29542, 19057, 31840}, {19057, 12441, 26052, 15902, 514, 25666, 29421, 27600}, {29542, 31840, 11912, 28800, 2048, 2184, 10778, 10349}, {27600, 19057, 11912, 15902, 2184, 29421, 10806, 31840}, {26052, 2048, 25666, 10349, 29542, 12441, 28800, 514}, {10778, 10806, 12441, 15902, 25666, 27600, 31840, 28800}, {2184, 10349, 19057, 26052, 514, 29421, 29542, 11912}, {514, 10349, 31840, 15902, 10778, 29542, 26052, 2184}, {27600, 12441, 2048, 29421, 19057, 28800, 25666, 10806}, {28800, 10778, 19057, 15902, 29421, 31840, 2048, 10349}, {12441, 514, 27600, 11912, 2184, 10806, 26052, 25666}, {12441, 2184, 29542, 15902, 10349, 11912, 25666, 19057}, {2048, 27600, 26052, 10778, 31840, 514, 10806, 28800}, {29542, 2048, 10806, 15902, 27600, 28800, 10349, 26052}, {29421, 11912, 10778, 12441, 25666, 31840, 2184, 19057}, {10349, 28800, 27600, 15902, 10806, 2048, 29542, 514}, {19057, 2184, 31840, 25666, 12441, 10778, 11912, 29421}, {25666, 11912, 10349, 15902, 29542, 2184, 12441, 29421}, {28800, 10806, 514, 31840, 10778, 26052, 27600, 2048}, {2048, 31840, 29421, 15902, 19057, 10778, 28800, 29542}, {25666, 26052, 10806, 2184, 11912, 27600, 514, 12441}, {26052, 29542, 10778, 15902, 31840, 10349, 514, 11912}, {10806, 25666, 28800, 19057, 29421, 2048, 12441, 27600}, {31840, 27600, 25666, 15902, 12441, 10806, 10778, 2048}, {11912, 29542, 29421, 514, 26052, 19057, 10349, 2184}, {10806, 29421, 2184, 15902, 11912, 19057, 27600, 10778}, {514, 28800, 12441, 29542, 10349, 25666, 2048, 26052}, {29421, 25666, 514, 15902, 26052, 12441, 19057, 10806}, {10349, 10778, 2184, 2048, 28800, 11912, 31840, 29542}, {2184, 514, 2048, 15902, 28800, 26052, 11912, 12441}, {31840, 19057, 29542, 10806, 27600, 10349, 29421, 10778}
};

void polymul_128x128_rader_in(int16x8_t* a_out, int16x8_t* b_out, int16x8_t* a, int16x8_t* b){
    int16x8_t NTT_a[32], NTT_b[32];
    //Rader17: 8x2 = 16
    for (int i=0; i<2; i++){
        __truncated_rader17_128(NTT_a+i, a+i, &twiddle_p_128, &twiddle_n_128, twist128+i);
        __truncated_rader17_128(NTT_b+i, b+i, &twiddle_p_128, &twiddle_n_128, twist128+i);
    }
    
    for(int i = 0;i<16;i++){
        a_out[i] = barrett_fake(vaddq_s16(NTT_a[2*i], NTT_a[2*i+1]));
        a_out[i+16] = barrett_fake(vsubq_s16(NTT_a[2*i], NTT_a[2*i+1]));
        b_out[i] = barrett_fake(vaddq_s16(NTT_b[2*i], NTT_b[2*i+1]));
        b_out[i+16] = barrett_fake(vsubq_s16(NTT_b[2*i], NTT_b[2*i+1]));
    }

    for (int i=0; i<4; i++){
        __transpose8_8x8(a_out+(i*8));
        __transpose8_8x8(b_out+(i*8));
    }
}

void polymul_128x128_rader_16x16(int16x8_t* c, int16x8_t* a, int16x8_t* b){       
    __rader8x8p(c, a, b);
    __rader8x8p(c+8, a+8, b+8);
    __rader8x8n(c+16, a+16, b+16);
    __rader8x8n(c+24, a+24, b+24);
}

void polymul_128x128_rader_16x16_x1(int16x8_t* c, int16x8_t* a, int16x8_t* b){       
    __rader8x8p_x1(c, a, b);
    __rader8x8p_x1(c+8, a+8, b+8);
    __rader8x8n_x1(c+16, a+16, b+16);
    __rader8x8n_x1(c+24, a+24, b+24);
}

void polymul_128x128_rader_16x16_x4(int16x8_t* c, int16x8_t* a, int16x8_t* b){       
    __rader8x8p_x4(c, a, b);
    __rader8x8p_x4(c+8, a+8, b+8);
    __rader8x8n_x4(c+16, a+16, b+16);
    __rader8x8n_x4(c+24, a+24, b+24);
}

void polymul_128x128_rader_out(int16x8_t* c, int16x8_t* c_in){
    int16x8_t NTT_c[32];

    for (int i=0; i<4; i++){
        __transpose8_8x8(c_in+(i*8));
    }

    //without 1/2
    for(int i = 0;i<16;i++){
        NTT_c[2*i]   = (vaddq_s16(c_in[i], c_in[i+16]));
        NTT_c[2*i+1] = (vsubq_s16(c_in[i], c_in[i+16]));
    }
    //Rader17_inv
    for (int i=0; i<2; i++){
        __itruncated_rader_17_128(c+i, NTT_c+i, &itwiddle_p_128, &itwiddle_n_128, twist128_inv+i);
    }
}

void polymul_128x128_rader_out_x1(int16x8_t* c, int16x8_t* c_in){
    int16x8_t NTT_c[32];
    
    for (int i=0; i<4; i++){
        __transpose8_8x8(c_in+(i*8));
    }

    //without 1/2
    for(int i = 0;i<16;i++){
        NTT_c[2*i]   = (vaddq_s16(c_in[i], c_in[i+16]));
        NTT_c[2*i+1] = (vsubq_s16(c_in[i], c_in[i+16]));
    }

    //Rader17_inv
    for (int i=0; i<2; i++){
        __itruncated_rader_17_128(c+i, NTT_c+i, &itwiddle_p_128, &itwiddle_n_128, twist128_inv_x1+i);
    }
}

void polymul_128x128_rader_out_x4(int16x8_t* c, int16x8_t* c_in){
    int16x8_t NTT_c[32];
    
    for (int i=0; i<4; i++){
        __transpose8_8x8(c_in+(i*8));
    }

    //without 1/2
    for(int i = 0;i<16;i++){
        NTT_c[2*i]   = (vaddq_s16(c_in[i], c_in[i+16]));
        NTT_c[2*i+1] = (vsubq_s16(c_in[i], c_in[i+16]));
    }

    //Rader17_inv
    for (int i=0; i<2; i++){
        __itruncated_rader_17_128(c+i, NTT_c+i, &itwiddle_p_128, &itwiddle_n_128, twist128_inv_x4+i);
    }
}

// ---------------------------------------------------------------------

void polymul_128x128_karatsuba(int16x8_t *c, int16x8_t *a, int16x8_t *b)
{
    // low
    polymul_64x64(c, a, b);
    // high
    polymul_64x64(c + 16, a + 8, b + 8);
    // mid
    int16x8_t tmpa[8], tmpb[8], tmpc[16];
    for (int i = 0; i < 8; i++)
    {
        tmpa[i] = barrett_fake(vaddq_s16(a[i], a[i + 8]));
        tmpb[i] = barrett_fake(vaddq_s16(b[i], b[i + 8]));
    }
    polymul_64x64(tmpc, tmpa, tmpb);
    for (int i = 0; i < 16; i++)
    {
        tmpc[i] = (vsubq_s16(tmpc[i], vaddq_s16(c[i], c[i + 16])));
    }
    // add back to c
    for (int i = 0; i < 16; i++)
    {
        c[i + 8] = vaddq_s16(c[i + 8], tmpc[i]);
    }
    for (int i = 0; i < 32; i++)
    {
        c[i] = barrett_fake(c[i]);
    }
}

// ---------------------------------------------------------------------

void polymul_128x128_schoolbook(int16x8_t *c, int16x8_t *a, int16x8_t *b)
{
    int16x8_t c1[4][16];
    polymul_64x64(c1[0], a, b);
    polymul_64x64(c1[1], a, b+8);
    polymul_64x64(c1[2], a+8, b);
    polymul_64x64(c1[3], a+8, b+8);
    for(int i = 0;i<8;i++){
        c[i] = c1[0][i];
        c[i+8] = barrett_fake(c1[0][i+8] + c1[1][i] + c1[2][i]);
        c[i+16] = barrett_fake(c1[3][i] + c1[1][i+8] + c1[2][i+8]);
        c[i+24] = c1[3][i+8];
    }
}

// ---------------------------------------------------------------------

void polymul_128x128_Toom_InputTransform(int16x8_t *c, int16x8_t *a)
{
    //Input
    int16x8_t odd[16] = {0}, even[16] = {0};
    for (int i = 0; i < 4; i++)
    {
        // 0
        c[i] = (a[i]);

        // 00
        c[i+12] = (a[i+12]);

        // even/odd: 1, -1, 2, -2
        even[i] = (vaddq_s16(a[i], a[8+i]));
        odd[i] = (vaddq_s16(a[4+i], a[12+i]));
        even[i+4] = barrett_mla_4(a[i], a[8+i]);
        odd[i+4] = barrett_mla_8(barrett_mla_2(odd[i+4], a[4+i]), a[12+i]);
        
        // 4
        c[i+24] = barrett_fake(barrett_mla_64(barrett_mla_16(barrett_mla_4(a[i], a[4+i]), (a[8+i])), (a[12+i])));
    }

    for (int i=0; i<4; i++){
        //1, -1
        c[i+4] =  barrett_fake(vaddq_s16(even[i], odd[i]));
        c[i+16] = barrett_fake(vsubq_s16(even[i], odd[i])); 
        //2, -2
        c[i+8] =  barrett_fake(vaddq_s16(even[i+4], odd[i+4]));
        c[i+20] =  barrett_fake(vsubq_s16(even[i+4], odd[i+4]));
    }
}

void polymul_128x128_Toom_Out(int16x8_t *c, int16x8_t *h){
    int16x8_t cx[56] = {0};

    for(int i = 0;i<8;i++){
        cx[i] = (h[i]);

        cx[8+i] = barrett_mla_3443(cx[8+i], h[0+i]);
        cx[8+i] = barrett_mla_511(cx[8+i], h[8+i]);
        cx[8+i] = barrett_mla_765(cx[8+i], h[16+i]);
        cx[8+i] = barrett_mla_4575(cx[8+i], h[24+i]);
        cx[8+i] = barrett_mla_2448(cx[8+i], h[32+i]);
        cx[8+i] = barrett_mla_4336(cx[8+i], h[40+i]);
        cx[8+i] = barrett_mla_2270(cx[8+i], h[48+i]);

        cx[16+i] = barrett_mla_3442(cx[16+i], h[0+i]);
        cx[16+i] = barrett_mla_1531(cx[16+i], vaddq_s16(h[8+i],  h[32+i]));
        cx[16+i] = barrett_mla_1339(cx[16+i], vaddq_s16(h[16+i], h[40+i]));
        cx[16+i] = barrett_mla_4(cx[16+i], h[24+i]);

        cx[24+i] = barrett_mla_1435(cx[24+i], h[0+i]);
        cx[24+i] = barrett_mla_2040(cx[24+i], h[8+i]);
        cx[24+i] = barrett_mla_861(cx[24+i], h[16+i]);
        cx[24+i] = barrett_mla_20(cx[24+i], h[24+i]);
        cx[24+i] = barrett_mla_797(cx[24+i], h[40+i]);
        cx[24+i] = barrett_mla_4049(cx[24+i], h[48+i]);

        cx[32+i] = barrett_mla_1148(cx[32+i], h[0+i]);
        cx[32+i] = barrett_mla_765(cx[32+i], vaddq_s16(h[8+i], h[32+i]));
        cx[32+i] = barrett_mla_3252(cx[32+i], vaddq_s16(h[16+i],h[40+i]));
        cx[32+i] = barrett_mla_4586(cx[32+i], h[24+i]);

        cx[40+i] = barrett_mla_4304(cx[40+i], h[0+i]);
        cx[40+i] = barrett_mla_4336(cx[40+i], h[8+i]);
        cx[40+i] = barrett_mla_2965(cx[40+i], h[16+i]);
        cx[40+i] = barrett_mla_4587(cx[40+i], h[24+i]);
        cx[40+i] = barrett_mla_4438(cx[40+i], h[32+i]);
        cx[40+i] = barrett_mla_4049(cx[40+i], h[40+i]);
        cx[40+i] = barrett_mla_2863(cx[40+i], h[48+i]);
        
        cx[48+i] = (h[24+i]);
    }
    
    for (int i = 0; i < 7; i++)
    {
        for (int j = 0; j < 8; j++)
        {
            c[(i * 4) + j] = barrett_fake(vaddq_s16(c[(i * 4) + j], cx[(i * 8) + j]));
        }
    }
}

// ----------------------------------------------------

void SchonhageOut_128(int16x8_t* out, int16x8_t* in){
    int16x8_t tmp[64] = {0};

    //(y^2-1)(y^2+1)(y^2-x^8)(y^2+x^8)(y^2-x^4)(y^2+x^4)(y^2-x^12)(y^2+x^12)
    //(y^2-x^2)(y^2+x^2)(y^2-x^10)(y^2+x^10)(y^2-x^6)(y^2+x^6)(y^2-x^14)(y^2+x^14)
    //(y-1)(y+1)       -> (y^2-1)
    for (int i=0; i<2; i++){
        tmp[i]   = vaddq_s16(in[i], in[i+2]);
        tmp[i+2] = vsubq_s16(in[i], in[i+2]);
    }
    //(y-x^8)(y+x^8)   -> (y^2+1)
    tmp[4] = vaddq_s16(in[4], in[6]);
    tmp[5] = vaddq_s16(in[5], in[7]);
    tmp[6] = vsubq_s16(in[5], in[7]);
    tmp[7] = vsubq_s16(in[6], in[4]);
    //(y-x^4)(y+x^4)   -> (y^2-x^8)
    tmp[8]  = vaddq_s16(in[8], in[10]);
    tmp[9]  = vaddq_s16(in[9], in[11]);
    tmp[10] = vsubq_s16(vextq_s16(in[8], in[9], 4), vextq_s16(in[10], in[11], 4));
    tmp[11] = vsubq_s16(vextq_s16(in[9], vnegq_s16(in[8]), 4), vextq_s16(in[11], vnegq_s16(in[10]), 4));
    //(y-x^12)(y+x^12) -> (y^2+x^8)
    tmp[12] = vaddq_s16(in[12], in[14]);
    tmp[13] = vaddq_s16(in[13], in[15]);
    tmp[14] = vsubq_s16(vextq_s16(in[13], vnegq_s16(in[12]), 4), vextq_s16(in[15], vnegq_s16(in[14]), 4));
    tmp[15] = vsubq_s16(vextq_s16(in[14], in[15], 4), vextq_s16(in[12], in[13], 4));
    //(y-x^2)(y+x^2)   -> (y^2-x^4)
    tmp[16] = vaddq_s16(in[16], in[18]);
    tmp[17] = vaddq_s16(in[17], in[19]);
    tmp[18] = vsubq_s16(vextq_s16(in[16], in[17], 2), vextq_s16(in[18], in[19], 2));
    tmp[19] = vsubq_s16(vextq_s16(in[17], vnegq_s16(in[16]), 2), vextq_s16(in[19], vnegq_s16(in[18]), 2));
    //(y-x^10)(y+x^10) -> (y^2+x^4)
    tmp[20] = vaddq_s16(in[20], in[22]);
    tmp[21] = vaddq_s16(in[21], in[23]);
    tmp[22] = vsubq_s16(vextq_s16(in[21], vnegq_s16(in[20]), 2), vextq_s16(in[23], vnegq_s16(in[22]), 2));
    tmp[23] = vsubq_s16(vextq_s16(in[22], in[23], 2), vextq_s16(in[20], in[21], 2));
    //(y-x^6)(y+x^6)   -> (y^2-x^12)
    tmp[24] = vaddq_s16(in[24], in[26]);
    tmp[25] = vaddq_s16(in[25], in[27]);
    tmp[26] = vsubq_s16(vextq_s16(in[24], in[25], 6), vextq_s16(in[26], in[27], 6));
    tmp[27] = vsubq_s16(vextq_s16(in[25], vnegq_s16(in[24]), 6), vextq_s16(in[27], vnegq_s16(in[26]), 6));
    //(y-x^14)(y+x^14) -> (y^2+x^12)
    tmp[28] = vaddq_s16(in[28], in[30]);
    tmp[29] = vaddq_s16(in[29], in[31]);
    tmp[30] = vsubq_s16(vextq_s16(in[29], vnegq_s16(in[28]), 6), vextq_s16(in[31], vnegq_s16(in[30]), 6));
    tmp[31] = vsubq_s16(vextq_s16(in[30], in[31], 6), vextq_s16(in[28], in[29], 6));
    //(y-x)(y+x)       -> (y^2-x^2)
    tmp[32] = vaddq_s16(in[32], in[34]);
    tmp[33] = vaddq_s16(in[33], in[35]);
    tmp[34] = vsubq_s16(vextq_s16(in[32], in[33], 1), vextq_s16(in[34], in[35], 1));
    tmp[35] = vsubq_s16(vextq_s16(in[33], vnegq_s16(in[32]), 1), vextq_s16(in[35], vnegq_s16(in[34]), 1));
    //(y-x^9)(y-x^9)   -> (y^2+x^2)
    tmp[36] = vaddq_s16(in[36], in[38]);
    tmp[37] = vaddq_s16(in[37], in[39]);
    tmp[38] = vsubq_s16(vextq_s16(in[37], vnegq_s16(in[36]), 1), vextq_s16(in[39], vnegq_s16(in[38]), 1));
    tmp[39] = vsubq_s16(vextq_s16(in[38], in[39], 1), vextq_s16(in[36], in[37], 1));
    //(y-x^5)(y+x^5)   -> (y^2-x^10)
    tmp[40] = vaddq_s16(in[40], in[42]);
    tmp[41] = vaddq_s16(in[41], in[43]);
    tmp[42] = vsubq_s16(vextq_s16(in[40], in[41], 5), vextq_s16(in[42], in[43], 5));
    tmp[43] = vsubq_s16(vextq_s16(in[41], vnegq_s16(in[40]), 5), vextq_s16(in[43], vnegq_s16(in[42]), 5));
    //(y-x^13)(y+x^13) -> (y^2+x^10)
    tmp[44] = vaddq_s16(in[44], in[46]);
    tmp[45] = vaddq_s16(in[45], in[47]);
    tmp[46] = vsubq_s16(vextq_s16(in[45], vnegq_s16(in[44]), 5), vextq_s16(in[47], vnegq_s16(in[46]), 5));
    tmp[47] = vsubq_s16(vextq_s16(in[46], in[47], 5), vextq_s16(in[44], in[45], 5));
    //(y-x^3)(y+x^3)   -> (y^2-x^6)
    tmp[48] = vaddq_s16(in[48], in[50]);
    tmp[49] = vaddq_s16(in[49], in[51]);
    tmp[50] = vsubq_s16(vextq_s16(in[48], in[49], 3), vextq_s16(in[50], in[51], 3));
    tmp[51] = vsubq_s16(vextq_s16(in[49], vnegq_s16(in[48]), 3), vextq_s16(in[51], vnegq_s16(in[50]), 3));
    //(y-x^11)(y+x^11) -> (y^2+x^6)
    tmp[52] = vaddq_s16(in[52], in[54]);
    tmp[53] = vaddq_s16(in[53], in[55]);
    tmp[54] = vsubq_s16(vextq_s16(in[53], vnegq_s16(in[52]), 3), vextq_s16(in[55], vnegq_s16(in[54]), 3));
    tmp[55] = vsubq_s16(vextq_s16(in[54], in[55], 3), vextq_s16(in[52], in[53], 3));
    //(y-x^7)(y+x^7)   -> (y^2-x^14)
    tmp[56] = vaddq_s16(in[56], in[58]);
    tmp[57] = vaddq_s16(in[57], in[59]);
    tmp[58] = vsubq_s16(vextq_s16(in[56], in[57], 7), vextq_s16(in[58], in[59], 7));
    tmp[59] = vsubq_s16(vextq_s16(in[57], vnegq_s16(in[56]), 7), vextq_s16(in[59], vnegq_s16(in[58]), 7));
    //(y-x^15)(y+x^15) -> (y^2+x^14)
    tmp[60] = vaddq_s16(in[60], in[62]);
    tmp[61] = vaddq_s16(in[61], in[63]);
    tmp[62] = vsubq_s16(vextq_s16(in[61], vnegq_s16(in[60]), 7), vextq_s16(in[63], vnegq_s16(in[62]), 7));
    tmp[63] = vsubq_s16(vextq_s16(in[62], in[63], 7), vextq_s16(in[60], in[61], 7));

    //(y^4-1)(y^4+1)(y^4-x^8)(y^4+x^8)(y^4-x^4)(y^4+x^4)(y^4-x^12)(y^4+x^12)
    //(y^2-1)(y^2+1) -> (y^4-1)
    for (int i=0; i<4; i++){
        in[i]   = vaddq_s16(tmp[i], tmp[i+4]);
        in[i+4] = vsubq_s16(tmp[i], tmp[i+4]);
    }
    for (int i=0; i<4; i+=2){
        //(y^2-x^8)(y^2+x^8)   -> (y^4+1)
        in[8+i] = vaddq_s16(tmp[8+i], tmp[12+i]);
        in[9+i] = vaddq_s16(tmp[9+i], tmp[13+i]);
        in[12+i] = vsubq_s16(tmp[9+i], tmp[13+i]);
        in[13+i] = vsubq_s16(tmp[12+i], tmp[8+i]);
        //(y^2-x^4)(y^2+x^4)   -> (y^4-x^8)
        in[16+i]  = vaddq_s16(tmp[16+i], tmp[20+i]);
        in[17+i]  = vaddq_s16(tmp[17+i], tmp[21+i]);
        in[20+i] = vsubq_s16(vextq_s16(tmp[16+i], tmp[17+i], 4), vextq_s16(tmp[20+i], tmp[21+i], 4));
        in[21+i] = vsubq_s16(vextq_s16(tmp[17+i], vnegq_s16(tmp[16+i]), 4), vextq_s16(tmp[21+i], vnegq_s16(tmp[20+i]), 4));
        //(y^2-x^12)(y^2+x^12) -> (y^4+x^8)
        in[24+i] = vaddq_s16(tmp[24+i], tmp[28+i]);
        in[25+i] = vaddq_s16(tmp[25+i], tmp[29+i]);
        in[28+i] = vsubq_s16(vextq_s16(tmp[25+i], vnegq_s16(tmp[24+i]), 4), vextq_s16(tmp[29+i], vnegq_s16(tmp[28+i]), 4));
        in[29+i] = vsubq_s16(vextq_s16(tmp[28+i], tmp[29+i], 4), vextq_s16(tmp[24+i], tmp[25+i], 4));
        //(y^2-x^2)(y^2+x^2)   -> (y^4-x^4)
        in[32+i] = vaddq_s16(tmp[32+i], tmp[36+i]);
        in[33+i] = vaddq_s16(tmp[33+i], tmp[37+i]);
        in[36+i] = vsubq_s16(vextq_s16(tmp[32+i], tmp[33+i], 2), vextq_s16(tmp[36+i], tmp[37+i], 2));
        in[37+i] = vsubq_s16(vextq_s16(tmp[33+i], vnegq_s16(tmp[32+i]), 2), vextq_s16(tmp[37+i], vnegq_s16(tmp[36+i]), 2));
        //(y^2-x^10)(y^2+x^10) -> (y^4+x^4)
        in[40+i] = vaddq_s16(tmp[40+i], tmp[44+i]);
        in[41+i] = vaddq_s16(tmp[41+i], tmp[45+i]);
        in[44+i] = vsubq_s16(vextq_s16(tmp[41+i], vnegq_s16(tmp[40+i]), 2), vextq_s16(tmp[45+i], vnegq_s16(tmp[44+i]), 2));
        in[45+i] = vsubq_s16(vextq_s16(tmp[44+i], tmp[45+i], 2), vextq_s16(tmp[40+i], tmp[41+i], 2));
        //(y^2-x^6)(y^2+x^6)   -> (y^4-x^12)
        in[48+i] = vaddq_s16(tmp[48+i], tmp[52+i]);
        in[49+i] = vaddq_s16(tmp[49+i], tmp[53+i]);
        in[52+i] = vsubq_s16(vextq_s16(tmp[48+i], tmp[49+i], 6), vextq_s16(tmp[52+i], tmp[53+i], 6));
        in[53+i] = vsubq_s16(vextq_s16(tmp[49+i], vnegq_s16(tmp[48+i]), 6), vextq_s16(tmp[53+i], vnegq_s16(tmp[52+i]), 6));
        //(y^2-x^14)(y^2+x^14) -> (y^4+x^12)
        in[56+i] = vaddq_s16(tmp[56+i], tmp[60+i]);
        in[57+i] = vaddq_s16(tmp[57+i], tmp[61+i]);
        in[60+i] = vsubq_s16(vextq_s16(tmp[57+i], vnegq_s16(tmp[56+i]), 6), vextq_s16(tmp[61+i], vnegq_s16(tmp[60+i]), 6));
        in[61+i] = vsubq_s16(vextq_s16(tmp[60+i], tmp[61+i], 6), vextq_s16(tmp[56+i], tmp[57+i], 6));
    }

    //(y^4-1)(y^4+1)(y^4-x^8)(y^4+x^8)(y^4-x^4)(y^4+x^4)(y^4-x^12)(y^4+x^12)
    //(y^4-1)(y^4+1) -> (y^8-1)
    for (int i=0; i<8; i++){
        tmp[i]   = barrett_fake(in[i] + in[i+8]);
        tmp[i+8] = barrett_fake(in[i] - in[i+8]); 
    }
    //(y^4-x^8)(y^4+x^8)   -> (y^8+1)
    //(y^4-x^4)(y^4+x^4)   -> (y^8-x^8)
    //(y^4-x^12)(y^4+x^12) -> (y^8+x^8)
    for (int i=16; i<24; i++){
        tmp[i] = barrett_fake(in[i] + in[i+8]);
        tmp[i+16] = barrett_fake(in[i+16] + in[i+24]);
        tmp[i+32] = barrett_fake(in[i+32] + in[i+40]);
    }
    for (int i=0; i<4; i+=2){
        tmp[24+i] = barrett_fake(in[17+i] - in[25+i]); 
        tmp[25+i] = barrett_fake(in[24+i] - in[16+i]);
        tmp[28+i] = barrett_fake(in[21+i] - in[29+i]); 
        tmp[29+i] = barrett_fake(in[28+i] - in[20+i]);
        tmp[40+i] = barrett_fake(vextq_s16(in[32+i], in[33+i], 4) - vextq_s16(in[40+i], in[41+i], 4));
        tmp[41+i] = barrett_fake(vextq_s16(in[33+i], vnegq_s16(in[32+i]), 4) - vextq_s16(in[41+i], vnegq_s16(in[40+i]), 4));
        tmp[44+i] = barrett_fake(vextq_s16(in[36+i], in[37+i], 4) - vextq_s16(in[44+i], in[45+i], 4));
        tmp[45+i] = barrett_fake(vextq_s16(in[37+i], vnegq_s16(in[36+i]), 4) - vextq_s16(in[45+i], vnegq_s16(in[44+i]), 4));
        tmp[56+i] = barrett_fake(vextq_s16(in[49+i], vnegq_s16(in[48+i]), 4) - vextq_s16(in[57+i], vnegq_s16(in[56+i]), 4)); 
        tmp[57+i] = barrett_fake(vextq_s16(in[56+i], in[57+i], 4) - vextq_s16(in[48+i], in[49+i], 4));
        tmp[60+i] = barrett_fake(vextq_s16(in[53+i], vnegq_s16(in[52+i]), 4) - vextq_s16(in[61+i], vnegq_s16(in[60+i]), 4)); 
        tmp[61+i] = barrett_fake(vextq_s16(in[60+i], in[61+i], 4) - vextq_s16(in[52+i], in[53+i], 4));
    }

    //(y^8-1)(y^8+1)(y^8-x^8)(y^8+x^8)
    //(y^8-1)(y^8+1) -> (y^16-1)
    for (int i=0; i<16; i++){
        in[i]   = (tmp[i] + tmp[i+16]);
        in[i+16] = (tmp[i] - tmp[i+16]);
    }
    //(y^8-x^8)(y^8+x^8) -> (y^16+1)
    for (int i=32; i<48; i++){
        in[i] = (tmp[i] + tmp[i+16]);
    }
    for (int i=0; i<8; i++){
        in[48+i*2] = (tmp[33+i*2] - tmp[49+i*2]);
        in[49+i*2] = (tmp[48+i*2] - tmp[32+i*2]);
    }

    //(y^16-1)(y^16+1)
    //(y^16-1)(y^16+1) ->(y^32-1)
    for (int i=0; i<32; i++){
        tmp[i] = (in[i] + in[i+32]);
        tmp[i+32] = (in[i] - in[i+32]);
    }

    //y = x^8 [1/32]
    out[0] = innerProduct_2439(tmp[0] - tmp[63]);
    for (int i=0; i<31; i++){
        out[i+1] = innerProduct_2439(tmp[i*2+1] + tmp[i*2+2]);
    }
}

void SchonhageMul_128(int16x8_t* c, int16x8_t* a, int16x8_t* b){
    //32_16x16 -> (8_16x16, 8_16x16, 8_16x16, 8_16x16)
    int16x8_t res[128], tmpa[56] = {0};
    __asm8_16x16(res, a, b, tmpa);
    __asm8_16x16(res+32, a+16, b+16, tmpa);
    __asm8_16x16(res+64, a+32, b+32, tmpa);
    __asm8_16x16(res+96, a+48, b+48, tmpa);

    //mod (x^16+1)
    for (int i=0; i<32; i++){
        c[i*2] = (res[i*4] - res[i*4+2]);
        c[i*2+1] = (res[i*4+1] - res[i*4+3]);
    }
}

void SchonhageIn_128(int16x8_t* out, int16x8_t* in){
    int16x8_t tmp[64] = {0};
    //x^16 = -1
    //(y^32-1) = (y^16-1)(y^16+1)
    //========Do nothing========

    //(y^16-1)(y^16+1) = (y^8-1)(y^8+1)(y^8-x^8)(y^8+x^8)
    for (int i=0; i<8; i++){
        //(y^16-1) -> (y^8-1)(y^8+1)
        tmp[i*2]   = barrett_fake(vaddq_s16(in[i*2], in[i*2+16])); 
        tmp[i*2+16] = barrett_fake(vsubq_s16(in[i*2], in[i*2+16]));
        //(y^16+1) -> (y^8-x^8)(y^8+x^8)
        tmp[i*2+32] = barrett_fake(in[i*2]);
        tmp[i*2+33] = barrett_fake(in[i*2+16]);
        tmp[i*2+48] = barrett_fake(in[i*2]);
        tmp[i*2+49] = barrett_fake(vnegq_s16(in[i*2+16]));
    }

    //(y^8-1)(y^8+1)(y^8-x^8)(y^8+x^8) = (y^4-1)(y^4+1)(y^4-x^8)(y^4+x^8)(y^4-x^4)(y^4+x^4)(y^4-x^12)(y^4+x^12)
    for (int i=0; i<4; i++){
        //(y^4-x^8) -> (y^2-x^4)(y^2+x^4)
        in[i*2+32] = vaddq_s16(tmp[i*2+32], vextq_s16(vnegq_s16(tmp[i*2+41]), tmp[i*2+40], 4));
        in[i*2+33] = vaddq_s16(tmp[i*2+33], vextq_s16(tmp[i*2+40], tmp[i*2+41], 4));
        in[i*2+40] = vsubq_s16(tmp[i*2+32], vextq_s16(vnegq_s16(tmp[i*2+41]), tmp[i*2+40], 4));
        in[i*2+41] = vsubq_s16(tmp[i*2+33], vextq_s16(tmp[i*2+40], tmp[i*2+41], 4));
        //(y^4+x^8) -> (y^2-x^12)(y^2+x^12)
        in[i*2+48] = tmp[i*2+48] - vextq_s16(tmp[i*2+56], tmp[i*2+57], 4);
        in[i*2+49] = tmp[i*2+49] + vextq_s16(vnegq_s16(tmp[i*2+57]), tmp[i*2+56], 4);
        in[i*2+56] = tmp[i*2+48] + vextq_s16(tmp[i*2+56], tmp[i*2+57], 4);
        in[i*2+57] = tmp[i*2+49] - vextq_s16(vnegq_s16(tmp[i*2+57]), tmp[i*2+56], 4);

        //(y^8-1)   -> (y^4-1)(y^4+1)
        in[i*2]   = vaddq_s16(tmp[i*2], tmp[i*2+8]);
        in[i*2+8] = vsubq_s16(tmp[i*2], tmp[i*2+8]);
        //(y^8+1)   -> (y^4-x^8)(y^4+x^8)
        in[i*2+16]  = tmp[i*2+16];
        in[i*2+17]  = tmp[i*2+24];
        in[i*2+24] = tmp[i*2+16];
        in[i*2+25] = vnegq_s16(tmp[i*2+24]);
    }
    
    //(y^4-1)(y^4+1)(y^4-x^8)(y^4+x^8)(y^4-x^4)(y^4+x^4)(y^4-x^12)(y^4+x^12)
    for (int i=0; i<4; i+=2){
        //(y^4-1)    -> (y^2-1)(y^2+1)
        tmp[0+i] = (in[0+i] + in[4+i]);
        tmp[4+i] = (in[0+i] - in[4+i]);
        tmp[1+i] = tmp[5+i] = v0;
        //(y^4+1)    -> (y^2-x^8)(y^2+x^8)
        tmp[8+i]  = tmp[12+i] = in[8+i];
        tmp[9+i]  = in[12+i];
        tmp[13+i]  = vnegq_s16(in[12+i]);
        //(y^4-x^8)  -> (y^2-x^4)(y^2+x^4)
        tmp[16+i]  = (vaddq_s16(in[16+i], vextq_s16(vnegq_s16(in[21+i]), in[20+i], 4)));
        tmp[17+i]  = (vaddq_s16(in[17+i], vextq_s16(in[20+i], in[21+i], 4)));
        tmp[20+i]  = (vsubq_s16(in[16+i], vextq_s16(vnegq_s16(in[21+i]), in[20+i], 4)));
        tmp[21+i]  = (vsubq_s16(in[17+i], vextq_s16(in[20+i], in[21+i], 4)));
        //(y^4+x^8)  -> (y^2-x^12)(y^2+x^12)
        tmp[24+i] = (vsubq_s16(in[24+i], vextq_s16(in[28+i], in[29+i], 4)));
        tmp[25+i] = (vaddq_s16(in[25+i], vextq_s16(vnegq_s16(in[29+i]), in[28+i], 4)));
        tmp[28+i] = (vaddq_s16(in[24+i], vextq_s16(in[28+i], in[29+i], 4)));
        tmp[29+i] = (vsubq_s16(in[25+i], vextq_s16(vnegq_s16(in[29+i]), in[28+i], 4)));
        //(y^4-x^4)  -> (y^2-x^2)(y^2+x^2)
        tmp[32+i] = (vaddq_s16(in[32+i], vextq_s16(vnegq_s16(in[37+i]), in[36+i], 6)));
        tmp[33+i] = (vaddq_s16(in[33+i], vextq_s16(in[36+i], in[37+i], 6)));
        tmp[36+i] = (vsubq_s16(in[32+i], vextq_s16(vnegq_s16(in[37+i]), in[36+i], 6)));
        tmp[37+i] = (vsubq_s16(in[33+i], vextq_s16(in[36+i], in[37+i], 6)));
        //(y^4+x^4)  -> (y^2-x^10)(y^2+x^10)
        tmp[40+i] = (vsubq_s16(in[40+i], vextq_s16(in[44+i], in[45+i], 6)));
        tmp[41+i] = (vaddq_s16(in[41+i], vextq_s16(vnegq_s16(in[45+i]), in[44+i], 6)));
        tmp[44+i] = (vaddq_s16(in[40+i], vextq_s16(in[44+i], in[45+i], 6)));
        tmp[45+i] = (vsubq_s16(in[41+i], vextq_s16(vnegq_s16(in[45+i]), in[44+i], 6)));
        //(y^4-x^12) -> (y^2-x^6)(y^2+x^6)
        tmp[48+i] = (vaddq_s16(in[48+i], vextq_s16(vnegq_s16(in[53+i]), in[52+i], 2)));
        tmp[49+i] = (vaddq_s16(in[49+i], vextq_s16(in[52+i], in[53+i], 2)));
        tmp[52+i] = (vsubq_s16(in[48+i], vextq_s16(vnegq_s16(in[53+i]), in[52+i], 2)));
        tmp[53+i] = (vsubq_s16(in[49+i], vextq_s16(in[52+i], in[53+i], 2)));
        //(y^4+x^12) -> (y^2-x^14)(y^2+x^14)
        tmp[56+i] = (vsubq_s16(in[56+i], vextq_s16(in[60+i], in[61+i], 2)));
        tmp[57+i] = (vaddq_s16(in[57+i], vextq_s16(vnegq_s16(in[61+i]), in[60+i], 2)));
        tmp[60+i] = (vaddq_s16(in[56+i], vextq_s16(in[60+i], in[61+i], 2)));
        tmp[61+i] = (vsubq_s16(in[57+i], vextq_s16(vnegq_s16(in[61+i]), in[60+i], 2)));
    }
    
    //(y^2-1)(y^2+1)(y^2-x^8)(y^2+x^8)(y^2-x^4)(y^2+x^4)(y^2-x^12)(y^2+x^12)
    //(y^2-x^2)(y^2+x^2)(y^2-x^10)(y^2+x^10)(y^2-x^6)(y^2+x^6)(y^2-x^14)(y^2+x^14)
    //(y^2-1) -> (y-1)(y+1)
    for (int i=0; i<2; i++){
        out[i] = (tmp[i] + tmp[i+2]);
        out[i+2] = (tmp[i] - tmp[i+2]);
    }
    //(y^2+1) -> (y-x^8)(y+x^8)
    out[4]  = out[6] = tmp[4];
    out[5]  = tmp[6];
    out[7]  = vnegq_s16(tmp[6]);
    //(y^2-x^8) -> (y-x^4)(y+x^4)
    out[8]  = (vaddq_s16(tmp[8], vextq_s16(vnegq_s16(tmp[11]), tmp[10], 4)));
    out[9]  = (vaddq_s16(tmp[9], vextq_s16(tmp[10], tmp[11], 4)));
    out[10] = (vsubq_s16(tmp[8], vextq_s16(vnegq_s16(tmp[11]), tmp[10], 4)));
    out[11] = (vsubq_s16(tmp[9], vextq_s16(tmp[10], tmp[11], 4)));
    //(y^2+x^8)  -> (y-x^12)(y+x^12)
    out[12] = (vsubq_s16(tmp[12], vextq_s16(tmp[14], tmp[15], 4)));
    out[13] = (vaddq_s16(tmp[13], vextq_s16(vnegq_s16(tmp[15]), tmp[14], 4)));
    out[14] = (vaddq_s16(tmp[12], vextq_s16(tmp[14], tmp[15], 4)));
    out[15] = (vsubq_s16(tmp[13], vextq_s16(vnegq_s16(tmp[15]), tmp[14], 4)));
    //(y^2-x^4)  -> (y-x^2)(y+x^2)
    out[16] = (vaddq_s16(tmp[16], vextq_s16(vnegq_s16(tmp[19]), tmp[18], 6)));
    out[17] = (vaddq_s16(tmp[17], vextq_s16(tmp[18], tmp[19], 6)));
    out[18] = (vsubq_s16(tmp[16], vextq_s16(vnegq_s16(tmp[19]), tmp[18], 6)));
    out[19] = (vsubq_s16(tmp[17], vextq_s16(tmp[18], tmp[19], 6)));
    //(y^2+x^4)  -> (y-x^10)(y+x^10)
    out[20] = (vsubq_s16(tmp[20], vextq_s16(tmp[22], tmp[23], 6)));
    out[21] = (vaddq_s16(tmp[21], vextq_s16(vnegq_s16(tmp[23]), tmp[22], 6)));
    out[22] = (vaddq_s16(tmp[20], vextq_s16(tmp[22], tmp[23], 6)));
    out[23] = (vsubq_s16(tmp[21], vextq_s16(vnegq_s16(tmp[23]), tmp[22], 6)));
    //(y^2-x^12) -> (y-x^6)(y+x^6)
    out[24] = (vaddq_s16(tmp[24], vextq_s16(vnegq_s16(tmp[27]), tmp[26], 2)));
    out[25] = (vaddq_s16(tmp[25], vextq_s16(tmp[26], tmp[27], 2)));
    out[26] = (vsubq_s16(tmp[24], vextq_s16(vnegq_s16(tmp[27]), tmp[26], 2)));
    out[27] = (vsubq_s16(tmp[25], vextq_s16(tmp[26], tmp[27], 2)));
    //(y^2+x^12) -> (y-x^14)(y+x^14)
    out[28] = (vsubq_s16(tmp[28], vextq_s16(tmp[30], tmp[31], 2)));
    out[29] = (vaddq_s16(tmp[29], vextq_s16(vnegq_s16(tmp[31]), tmp[30], 2)));
    out[30] = (vaddq_s16(tmp[28], vextq_s16(tmp[30], tmp[31], 2)));
    out[31] = (vsubq_s16(tmp[29], vextq_s16(vnegq_s16(tmp[31]), tmp[30], 2)));
    //(y^2-x^2)  -> (y-x)(y+x)
    out[32] = (vaddq_s16(tmp[32], vextq_s16(vnegq_s16(tmp[35]), tmp[34], 7)));
    out[33] = (vaddq_s16(tmp[33], vextq_s16(tmp[34], tmp[35], 7)));
    out[34] = (vsubq_s16(tmp[32], vextq_s16(vnegq_s16(tmp[35]), tmp[34], 7)));
    out[35] = (vsubq_s16(tmp[33], vextq_s16(tmp[34], tmp[35], 7)));
    //(y^2+x^2)  -> (y-x^9)(y-x^9)
    out[36] = (vsubq_s16(tmp[36], vextq_s16(tmp[38], tmp[39], 7)));
    out[37] = (vaddq_s16(tmp[37], vextq_s16(vnegq_s16(tmp[39]), tmp[38], 7)));
    out[38] = (vaddq_s16(tmp[36], vextq_s16(tmp[38], tmp[39], 7)));
    out[39] = (vsubq_s16(tmp[37], vextq_s16(vnegq_s16(tmp[39]), tmp[38], 7)));
    //(y^2-x^10) -> (y-x^5)(y+x^5)
    out[40] = (vaddq_s16(tmp[40], vextq_s16(vnegq_s16(tmp[43]), tmp[42], 3)));
    out[41] = (vaddq_s16(tmp[41], vextq_s16(tmp[42], tmp[43], 3)));
    out[42] = (vsubq_s16(tmp[40], vextq_s16(vnegq_s16(tmp[43]), tmp[42], 3)));
    out[43] = (vsubq_s16(tmp[41], vextq_s16(tmp[42], tmp[43], 3)));
    //(y^2+x^10) -> (y-x^13)(y+x^13)
    out[44] = (vsubq_s16(tmp[44], vextq_s16(tmp[46], tmp[47], 3)));
    out[45] = (vaddq_s16(tmp[45], vextq_s16(vnegq_s16(tmp[47]), tmp[46], 3)));
    out[46] = (vaddq_s16(tmp[44], vextq_s16(tmp[46], tmp[47], 3)));
    out[47] = (vsubq_s16(tmp[45], vextq_s16(vnegq_s16(tmp[47]), tmp[46], 3)));
    //(y^2-x^6)  -> (y-x^3)(y+x^3)
    out[48] = (vaddq_s16(tmp[48], vextq_s16(vnegq_s16(tmp[51]), tmp[50], 5)));
    out[49] = (vaddq_s16(tmp[49], vextq_s16(tmp[50], tmp[51], 5)));
    out[50] = (vsubq_s16(tmp[48], vextq_s16(vnegq_s16(tmp[51]), tmp[50], 5)));
    out[51] = (vsubq_s16(tmp[49], vextq_s16(tmp[50], tmp[51], 5)));
    //(y^2+x^6)  -> (y-x^11)(y+x^11)
    out[52] = (vsubq_s16(tmp[52], vextq_s16(tmp[54], tmp[55], 5)));
    out[53] = (vaddq_s16(tmp[53], vextq_s16(vnegq_s16(tmp[55]), tmp[54], 5)));
    out[54] = (vaddq_s16(tmp[52], vextq_s16(tmp[54], tmp[55], 5)));
    out[55] = (vsubq_s16(tmp[53], vextq_s16(vnegq_s16(tmp[55]), tmp[54], 5)));
    //(y^2-x^14) -> (y-x^7)(y+x^7)
    out[56] = (vaddq_s16(tmp[56], vextq_s16(vnegq_s16(tmp[59]), tmp[58], 1)));
    out[57] = (vaddq_s16(tmp[57], vextq_s16(tmp[58], tmp[59], 1)));
    out[58] = (vsubq_s16(tmp[56], vextq_s16(vnegq_s16(tmp[59]), tmp[58], 1)));
    out[59] = (vsubq_s16(tmp[57], vextq_s16(tmp[58], tmp[59], 1)));
    //(y^2+x^14) -> (y-x^15)(y+x^15)
    out[60] = (vsubq_s16(tmp[60], vextq_s16(tmp[62], tmp[63], 1)));
    out[61] = (vaddq_s16(tmp[61], vextq_s16(vnegq_s16(tmp[63]), tmp[62], 1)));
    out[62] = (vaddq_s16(tmp[60], vextq_s16(tmp[62], tmp[63], 1)));
    out[63] = (vsubq_s16(tmp[61], vextq_s16(vnegq_s16(tmp[63]), tmp[62], 1)));
}

void permutation_128(int16x8_t* out, const int16x8_t* in){
    for (int i=0; i<16; i++){
        out[i*2] = in[i];
    }
}

// ----------------------------------------------------------------

void Brunn_128_stage256(int16x8_t *out, int16x8_t *in){
    // x^256-1  ->  (x^128-1) * (x^128+1)
    for(int i = 0;i<16;i++){
        out[i] = in[i];
        out[i+16] = in[i];
    }
}

void Brunn_128_stage128(int16x8_t *out, int16x8_t *in){
    // [0-31]  x^128-1  ->  (x^64-1) * (x^64+1)
    for(int i = 0;i<8;i++){
        out[i] = (vaddq_s16(in[i], in[i+8]));
        out[i+8] = (vsubq_s16(in[i], in[i+8]));
    }
    
    // [16-63] x^128+1  ->  (x^64- 1229 x^32+1) * (x^64+ 1229 x^32+1)
    // Butterfly 16/4 = 4
    int16x8_t butterfly[4][4] = {0};
    //int16x8_t alpha  = vdupq_n_s16(1229); // alpha^2-1 = 1
    for(int i = 0;i<4;i++){
        // a0-a2
        butterfly[0][i] = (vsubq_s16(in[i+16], in[i+16+8]));
        // a1 + (alpha^2-1)*a3
        butterfly[1][i] = (vaddq_s16(in[i+16+4], in[i+16+12]));
        // alpha*a2
        butterfly[2][i] = innerProduct_1229(in[i+16+8]);
        // alpha*a3
        butterfly[3][i] = innerProduct_1229(in[i+16+12]);
    }
    for(int i = 0;i<4;i++){
        // a2'
        out[i+16   ] = (vsubq_s16(butterfly[0][i], butterfly[3][i]));
        // a3'
        out[i+16+4 ] = (vaddq_s16(butterfly[1][i], butterfly[2][i]));
        // a0'
        out[i+16+8 ] = (vaddq_s16(butterfly[0][i], butterfly[3][i]));
        // a1'
        out[i+16+12] = (vsubq_s16(butterfly[1][i], butterfly[2][i]));
    }
}

void Bruun_128_stage64_butterfly_1229(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4][2] = {0};
    for(int i = 0;i<2;i++){
        // a0-a2
        butterfly[0][i] = (vsubq_s16(in[i], in[i+4]));
        // a1 + (alpha^2-1)*a3
        butterfly[1][i] = (vaddq_s16(in[i+2], in[i+6]));
        // alpha*a2
        butterfly[2][i] = innerProduct_1229(in[i+4]);
        // alpha*a3
        butterfly[3][i] = innerProduct_1229(in[i+6]);
    }
    for(int i = 0;i<2;i++){
        // a2'
        out[i  ] = barrett_fake(vsubq_s16(butterfly[0][i], butterfly[3][i]));
        // a3'
        out[i+2] = barrett_fake(vaddq_s16(butterfly[1][i], butterfly[2][i]));
        // a0'
        out[i+4] = barrett_fake(vaddq_s16(butterfly[0][i], butterfly[3][i]));
        // a1'
        out[i+6] = barrett_fake(vsubq_s16(butterfly[1][i], butterfly[2][i]));
    }
}

void Bruun_128_stage64_butterfly_2116(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4][2] = {0};
    for(int i = 0;i<2;i++){
        // a0-a2
        butterfly[0][i] = (vsubq_s16(in[i], in[i+4]));
        // a1 + (alpha^2-1)*a3
        butterfly[1][i] = barrett_mla_1230(in[i+2], in[i+6]);
        // alpha*a2
        butterfly[2][i] = innerProduct_2116(in[i+4]);
        // alpha*a3
        butterfly[3][i] = innerProduct_2116(in[i+6]);
    }
    for(int i = 0;i<2;i++){
        // a2'
        out[i  ] = barrett_fake(vsubq_s16(butterfly[0][i], butterfly[3][i]));
        // a3'
        out[i+2] = barrett_fake(vaddq_s16(butterfly[1][i], butterfly[2][i]));
        // a0'
        out[i+4] = barrett_fake(vaddq_s16(butterfly[0][i], butterfly[3][i]));
        // a1'
        out[i+6] = barrett_fake(vsubq_s16(butterfly[1][i], butterfly[2][i]));
    }
}

void Bruun_128_stage64_butterfly_58(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4][2] = {0};
    for(int i = 0;i<2;i++){
        // a0-a2
        butterfly[0][i] = (vsubq_s16(in[i], in[i+4]));
        // a1 + (alpha^2-1)*a3
        butterfly[1][i] = barrett_mla_3363(in[i+2], in[i+6]);
        // alpha*a2
        butterfly[2][i] = innerProduct_58(in[i+4]);
        // alpha*a3
        butterfly[3][i] = innerProduct_58(in[i+6]);
    }
    for(int i = 0;i<2;i++){
        // a2'
        out[i  ] = barrett_fake(vsubq_s16(butterfly[0][i], butterfly[3][i]));
        // a3'
        out[i+2] = barrett_fake(vaddq_s16(butterfly[1][i], butterfly[2][i]));
        // a0'
        out[i+4] = barrett_fake(vaddq_s16(butterfly[0][i], butterfly[3][i]));
        // a1'
        out[i+6] = barrett_fake(vsubq_s16(butterfly[1][i], butterfly[2][i]));
    }
}

void Brunn_128_stage64(int16x8_t *out, int16x8_t *in){
    // [0-15] x^64-1  ->  (x^32-1) * (x^32+1)
    for(int i = 0;i<4;i++){
        out[i] = barrett_fake(vaddq_s16(in[i], in[i+4]));
        out[i+4] = barrett_fake(vsubq_s16(in[i], in[i+4]));
    }
    // [16-31] x^64+1  ->  (x^32- 1229 x^16+1) * (x^32+ 1229 x^16+1)
    Bruun_128_stage64_butterfly_1229(out+8, in+8);
    // [16-47] x^64- 1229 x^32+1  ->  (x^32- 2116 x^16+1) * (x^32+ 2116 x^16+1)
    Bruun_128_stage64_butterfly_2116(out+16, in+16);
    // [48-63] x^64+ 1229 x^32+1  ->  (x^32- 58 x^16+1) * (x^32+ 58 x^16+1)
    Bruun_128_stage64_butterfly_58(out+24, in+24);
}

void Bruun_128_stage32_butterfly_1229(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4] = {0};
    // a0-a2
    butterfly[0] = (vsubq_s16(in[0], in[2]));
    // a1 + (alpha^2+1)*a3
    butterfly[1] = (vaddq_s16(in[1], in[3]));
    // alpha*a2
    butterfly[2] = innerProduct_1229(in[2]);
    // alpha*a3
    butterfly[3] = innerProduct_1229(in[3]);

    // a2'
    out[0] = barrett_fake(vsubq_s16(butterfly[0], butterfly[3]));
    // a3'
    out[1] = barrett_fake(vaddq_s16(butterfly[1], butterfly[2]));
    // a0'
    out[2] = barrett_fake(vaddq_s16(butterfly[0], butterfly[3]));
    // a1'
    out[3] = barrett_fake(vsubq_s16(butterfly[1], butterfly[2]));
}

void Bruun_128_stage32_butterfly_2116(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4] = {0};
    // a0-a2
    butterfly[0] = (vsubq_s16(in[0], in[2]));
    // a1 + (alpha^2+1)*a3
    butterfly[1] = barrett_mla_1230(in[1], in[3]);
    // alpha*a2
    butterfly[2] = innerProduct_2116(in[2]);
    // alpha*a3
    butterfly[3] = innerProduct_2116(in[3]);

    // a2'
    out[0] = barrett_fake(vsubq_s16(butterfly[0], butterfly[3]));
    // a3'
    out[1] = barrett_fake(vaddq_s16(butterfly[1], butterfly[2]));
    // a0'
    out[2] = barrett_fake(vaddq_s16(butterfly[0], butterfly[3]));
    // a1'
    out[3] = barrett_fake(vsubq_s16(butterfly[1], butterfly[2]));
}

void Bruun_128_stage32_butterfly_58(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4] = {0};
    // a0-a2
    butterfly[0] = (vsubq_s16(in[0], in[2]));
    // a1 + (alpha^2+1)*a3
    butterfly[1] = barrett_mla_3363(in[1], in[3]);
    // alpha*a2
    butterfly[2] = innerProduct_58(in[2]);
    // alpha*a3
    butterfly[3] = innerProduct_58(in[3]);

    // a2'
    out[0] = barrett_fake(vsubq_s16(butterfly[0], butterfly[3]));
    // a3'
    out[1] = barrett_fake(vaddq_s16(butterfly[1], butterfly[2]));
    // a0'
    out[2] = barrett_fake(vaddq_s16(butterfly[0], butterfly[3]));
    // a1'
    out[3] = barrett_fake(vsubq_s16(butterfly[1], butterfly[2]));
}

void Bruun_128_stage32_butterfly_neg_820(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4] = {0};
    
    // a0+a2
    butterfly[0] = (vaddq_s16(in[0], in[2]));
    // a1 + (alpha^2+1)*a3
    butterfly[1] = barrett_mla_2115(in[1], in[3]); 
    // alpha*a2
    butterfly[2] = innerProduct_820(in[2]);
    // alpha*a3
    butterfly[3] = innerProduct_3771(in[3]);
    
    // a2'
    out[0] = barrett_fake(vsubq_s16(butterfly[0], butterfly[3]));
    // a3'
    out[1] = barrett_fake(vaddq_s16(butterfly[1], butterfly[2]));
    // a0'
    out[2] = barrett_fake(vaddq_s16(butterfly[0], butterfly[3]));
    // a1'
    out[3] = barrett_fake(vsubq_s16(butterfly[1], butterfly[2]));
}

void Bruun_128_stage32_butterfly_neg_1243(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4] = {0};
    
    // a0+a2
    butterfly[0] = (vaddq_s16(in[0], in[2]));
    // a1 + (alpha^2+1)*a3
    butterfly[1] = barrett_mla_2474(in[1], in[3]);
    // alpha*a2
    butterfly[2] = innerProduct_1243(in[2]);
    // alpha*a3
    butterfly[3] = innerProduct_3348(in[3]);
    
    // a2'
    out[0] = barrett_fake(vsubq_s16(butterfly[0], butterfly[3]));
    // a3'
    out[1] = barrett_fake(vaddq_s16(butterfly[1], butterfly[2]));
    // a0'
    out[2] = barrett_fake(vaddq_s16(butterfly[0], butterfly[3]));
    // a1'
    out[3] = barrett_fake(vsubq_s16(butterfly[1], butterfly[2]));
}

void Bruun_128_stage32_butterfly_neg_1698(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4] = {0};
    
    // a0+a2
    butterfly[0] = (vaddq_s16(in[0], in[2]));
    // a1 + (alpha^2+1)*a3
    butterfly[1] = barrett_mla_57(in[1], in[3]);
    // alpha*a2
    butterfly[2] = innerProduct_1698(in[2]);
    // alpha*a3
    butterfly[3] = innerProduct_2893(in[3]);
    
    // a2'
    out[0] = barrett_fake(vsubq_s16(butterfly[0], butterfly[3]));
    // a3'
    out[1] = barrett_fake(vaddq_s16(butterfly[1], butterfly[2]));
    // a0'
    out[2] = barrett_fake(vaddq_s16(butterfly[0], butterfly[3]));
    // a1'
    out[3] = barrett_fake(vsubq_s16(butterfly[1], butterfly[2]));
}

void Bruun_128_stage32_butterfly_neg_542(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis.
    int16x8_t butterfly[4] = {0};
    
    // a0+a2
    butterfly[0] = (vaddq_s16(in[0], in[2]));
    // a1 + (alpha^2+1)*a3
    butterfly[1] = barrett_mla_4532(in[1], in[3]);
    // alpha*a2
    butterfly[2] = innerProduct_542(in[2]);
    // alpha*a3
    butterfly[3] = innerProduct_4049(in[3]);
    
    // a2'
    out[0] = barrett_fake(vsubq_s16(butterfly[0], butterfly[3]));
    // a3'
    out[1] = barrett_fake(vaddq_s16(butterfly[1], butterfly[2]));
    // a0'
    out[2] = barrett_fake(vaddq_s16(butterfly[0], butterfly[3]));
    // a1'
    out[3] = barrett_fake(vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_stage32(int16x8_t *out, int16x8_t *in){
    // [0-7] x^32-1  ->  (x^16-1) * (x^16+1)
    for(int i = 0;i<2;i++){
        out[i] = barrett_fake(vaddq_s16(in[i], in[i+2]));
        out[i+2] = barrett_fake(vsubq_s16(in[i], in[i+2]));
    }
    // [8-15] x^32+1  ->  (x^16- 1229 x^8+1) * (x^16+ 1229 x^8+1)
    Bruun_128_stage32_butterfly_1229(out+4,  in+4);
    // [16-23] x^32- 1229 x^16+1  ->  (x^16- 2116 x^8+1) * (x^16+ 2116 x^8+1)
    Bruun_128_stage32_butterfly_2116(out+8, in+8);   
    // [24-31] x^32+ 1229 x^16+1  ->  (x^16- 58 x^8+1) * (x^16+ 58 x^8+1)
    Bruun_128_stage32_butterfly_58(out+12, in+12);
    // [16-39] x^32- 2116 x^16+1  ->  (x^16- 820 x^8+1) * (x^16+ 820 x^8+1)
    Bruun_128_stage32_butterfly_neg_820(out+16, in+16);
    // [40-47] x^32+ 2116 x^16+1  ->  (x^16- 1243 x^8+1) * (x^16+ 1243 x^8+1)
    Bruun_128_stage32_butterfly_neg_1243(out+20, in+20);  
    // [48-55] x^32- 58 x^16+1    ->  (x^16- 1698 x^8+1) * (x^16+ 1698 x^8+1)
    Bruun_128_stage32_butterfly_neg_1698(out+24, in+24); 
    // [56-63] x^32+ 58 x^16+1    ->  (x^16- 542 x^8+1) * (x^16+ 542 x^8+1)
    Bruun_128_stage32_butterfly_neg_542(out+28, in+28); 
}

void Brunn_128_modRing_butterfly_1229(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    // [0-3]: [0-7]
    butterfly[0] = (vsubq_s16(in[0], in[2]));
    butterfly[1] = (vaddq_s16(in[1], in[3]));
    butterfly[2] = innerProduct_1229(in[2]);
    butterfly[3] = innerProduct_1229(in[3]);

    out[0] = (vsubq_s16(butterfly[0], butterfly[3]));
    out[1] = (vaddq_s16(butterfly[1], butterfly[2]));
    
    // [4-7]: [8-15]
    butterfly[0] = (vsubq_s16(in[4], in[6]));
    butterfly[1] = (vaddq_s16(in[5], in[7]));
    butterfly[2] = innerProduct_1229(in[6]);
    butterfly[3] = innerProduct_1229(in[7]);

    out[2] = (vaddq_s16(butterfly[0], butterfly[3]));
    out[3] = (vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_modRing_butterfly_2116(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    // [0-3]: [0-7]
    butterfly[0] = (vsubq_s16(in[0], in[2]));
    butterfly[1] = barrett_mla_1230(in[1], in[3]);
    butterfly[2] = innerProduct_2116(in[2]);
    butterfly[3] = innerProduct_2116(in[3]);

    out[0] = (vsubq_s16(butterfly[0], butterfly[3]));
    out[1] = (vaddq_s16(butterfly[1], butterfly[2]));
    
    // [4-7]: [8-15]
    butterfly[0] = (vsubq_s16(in[4], in[6]));
    butterfly[1] = barrett_mla_1230(in[5], in[7]);
    butterfly[2] = innerProduct_2116(in[6]);
    butterfly[3] = innerProduct_2116(in[7]);

    out[2] = (vaddq_s16(butterfly[0], butterfly[3]));
    out[3] = (vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_modRing_butterfly_58(int16x8_t *out, int16x8_t *in){  
    int16x8_t butterfly[4] = {0};
    // [0-3]: [0-7]
    butterfly[0] = (vsubq_s16(in[0], in[2]));
    butterfly[1] = barrett_mla_3363(in[1], in[3]);
    butterfly[2] = innerProduct_58(in[2]);
    butterfly[3] = innerProduct_58(in[3]);

    out[0] = (vsubq_s16(butterfly[0], butterfly[3]));
    out[1] = (vaddq_s16(butterfly[1], butterfly[2]));
    
    // [4-7]: [8-15]
    butterfly[0] = (vsubq_s16(in[4], in[6]));
    butterfly[1] = barrett_mla_3363(in[5], in[7]);
    butterfly[2] = innerProduct_58(in[6]);
    butterfly[3] = innerProduct_58(in[7]);

    out[2] = (vaddq_s16(butterfly[0], butterfly[3]));
    out[3] = (vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_modRing_butterfly_neg_820(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = vaddq_s16(in[0], in[2]);
    butterfly[1] = barrett_mla_2115(in[1], in[3]);
    butterfly[2] = innerProduct_820(in[2]);
    butterfly[3] = vnegq_s16(innerProduct_820(in[3]));
    out[0] = (vsubq_s16(butterfly[0], butterfly[3]));
    out[1] = (vaddq_s16(butterfly[1], butterfly[2]));
    butterfly[0] = vaddq_s16(in[4], in[6]);
    butterfly[1] = barrett_mla_2115(in[5], in[7]);
    butterfly[2] = innerProduct_820(in[6]);
    butterfly[3] = vnegq_s16(innerProduct_820(in[7]));
    out[2] = (vaddq_s16(butterfly[0], butterfly[3]));
    out[3] = (vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_modRing_butterfly_neg_1243(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = vaddq_s16(in[0], in[2]);
    butterfly[1] = barrett_mla_2474(in[1], in[3]);
    butterfly[2] = innerProduct_1243(in[2]);
    butterfly[3] = vnegq_s16(innerProduct_1243(in[3]));
    out[0] = (vsubq_s16(butterfly[0], butterfly[3]));
    out[1] = (vaddq_s16(butterfly[1], butterfly[2]));
    butterfly[0] = vaddq_s16(in[4], in[6]);
    butterfly[1] = barrett_mla_2474(in[5], in[7]);
    butterfly[2] = innerProduct_1243(in[6]);
    butterfly[3] = vnegq_s16(innerProduct_1243(in[7]));
    out[2] = (vaddq_s16(butterfly[0], butterfly[3]));
    out[3] = (vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_modRing_butterfly_neg_1698(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};

    butterfly[0] = vaddq_s16(in[0], in[2]);
    butterfly[1] = barrett_mla_57(in[1], in[3]);
    butterfly[2] = innerProduct_1698(in[2]);
    butterfly[3] = vnegq_s16(innerProduct_1698(in[3]));
    
    out[0] = (vsubq_s16(butterfly[0], butterfly[3]));
    out[1] = (vaddq_s16(butterfly[1], butterfly[2]));
    butterfly[0] = vaddq_s16(in[4], in[6]);
    butterfly[1] = barrett_mla_57(in[5], in[7]);
    butterfly[2] = innerProduct_1698(in[6]);
    butterfly[3] = vnegq_s16(innerProduct_1698(in[7]));
    out[2] = (vaddq_s16(butterfly[0], butterfly[3]));
    out[3] = (vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_modRing_butterfly_neg_542(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    
    butterfly[0] = vaddq_s16(in[0], in[2]);
    butterfly[1] = barrett_mla_4532(in[1], in[3]);
    butterfly[2] = innerProduct_542(in[2]);
    butterfly[3] = vnegq_s16(innerProduct_542(in[3]));
    out[0] = (vsubq_s16(butterfly[0], butterfly[3]));
    out[1] = (vaddq_s16(butterfly[1], butterfly[2]));
    
    butterfly[0] = vaddq_s16(in[4], in[6]);
    butterfly[1] = barrett_mla_4532(in[5], in[7]);
    butterfly[2] = innerProduct_542(in[6]);
    butterfly[3] = vnegq_s16(innerProduct_542(in[7]));
    out[2] = (vaddq_s16(butterfly[0], butterfly[3]));
    out[3] = (vsubq_s16(butterfly[1], butterfly[2]));
}

void Brunn_128_modRing(int16x8_t *out, int16x8_t *in){
    // out [0-3] = [0-7] mod (x^16-1)
    // out [4-7] = [8-15] mod (x^16+1)
    for(int i = 0;i<2;i++){
        out[i] = (vaddq_s16(in[i], in[i+2]));
        out[i+2] = (vsubq_s16(in[i+4], in[i+6]));
    }
    Brunn_128_modRing_butterfly_1229(out+4, in+8);
    Brunn_128_modRing_butterfly_2116(out+8, in+16);
    Brunn_128_modRing_butterfly_58(out+12, in+24);
    Brunn_128_modRing_butterfly_neg_820(out+16, in+32);
    Brunn_128_modRing_butterfly_neg_1243(out+20, in+40);
    Brunn_128_modRing_butterfly_neg_1698(out+24, in+48);
    Brunn_128_modRing_butterfly_neg_542(out+28, in+56);
}

void iBrunn_128_stage256(int16x8_t *out, int16x8_t *in){
    // (x^256-1) * (x^256+1)  ->  x^512-1
    for(int i = 0;i<16;i++){
        out[i] = barrett_fake(vaddq_s16(in[i], in[i+16]));
        out[i+16] = barrett_fake(vsubq_s16(in[i], in[i+16]));
    }
}

void iBrunn_128_stage128(int16x8_t *out, int16x8_t *in){
    // [0-31]   (x^128-1) * (x^128+1)  ->  x^256-1
    for(int i = 0;i<8;i++){
        out[i] = (vaddq_s16(in[i], in[i+8]));
        out[i+8] = (vsubq_s16(in[i], in[i+8]));
    }

    // [16-63] (x^128- 1229 x^64+1) * (x^128+ 1229 x^64+1) ->  x^256+1  
    // Caution: Positions are different from Thesis. 
    int16x8_t butterfly[4][4] = {0};
    for(int i = 0;i<4;i++){
        // a0 + a2
        butterfly[0][i] = (vaddq_s16(in[i+24], in[i+16]));
        // a1 + a3
        butterfly[1][i] = (vaddq_s16(in[i+16+12], in[i+16+4]));
        // a0 - a2
        butterfly[2][i] = (vsubq_s16(in[i+16+8], in[i+16+0]));
        // a3 - a1
        butterfly[3][i] = (vsubq_s16(in[i+16+4], in[i+16+12]));
    }
    for(int i = 0;i<4;i++){
        // a0
        out[i+16  ] = barrett_mla_2910(butterfly[0][i], butterfly[3][i]);
        // a1
        out[i+20] = barrett_mls_2910(butterfly[1][i], butterfly[2][i]);
        // a2
        out[i+24] = innerProduct_2910(butterfly[3][i]);
        // a3
        out[i+28] = innerProduct_2910(butterfly[2][i]);
    }

}

void iBruun_128_stage64_butterfly_2910(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4][2] = {0};
    for(int i = 0;i<2;i++){
        // a0 + a2
        butterfly[0][i] = (vaddq_s16(in[i+4], in[i+0]));
        // a1 + a3
        butterfly[1][i] = (vaddq_s16(in[i+6], in[i+2]));
        // a0 - a2
        butterfly[2][i] = (vsubq_s16(in[i+4], in[i+0]));
        // a3 - a1
        butterfly[3][i] = (vsubq_s16(in[i+2], in[i+6]));
    }
    for(int i = 0;i<2;i++){
        // a0
        out[i  ] = barrett_mla_2910(butterfly[0][i], butterfly[3][i]);
        // a1
        out[i+2] = barrett_mls_2910(butterfly[1][i], butterfly[2][i]);
        // a2
        out[i+4] = innerProduct_2910(butterfly[3][i]);
        // a3
        out[i+6] = innerProduct_2910(butterfly[2][i]);
    }
}

void iBruun_128_stage64_butterfly_1087(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4][2] = {0};
    for(int i = 0;i<2;i++){
        // a0 + a2
        butterfly[0][i] = (vaddq_s16(in[i+4], in[i+0]));
        // a1 + a3
        butterfly[1][i] = (vaddq_s16(in[i+6], in[i+2]));
        // a0 - a2
        butterfly[2][i] = (vsubq_s16(in[i+4], in[i+0]));
        // a3 - a1
        butterfly[3][i] = (vsubq_s16(in[i+2], in[i+6]));
    }
    for(int i = 0;i<2;i++){
        // a0
        out[i  ] = barrett_mla_1087(butterfly[0][i], butterfly[3][i]);
        // a1
        out[i+2] = barrett_mls_1029(butterfly[1][i], butterfly[2][i]);
        // a2
        out[i+4] = innerProduct_1087(butterfly[3][i]);
        // a3
        out[i+6] = innerProduct_1087(butterfly[2][i]);
    }
}

void iBruun_128_stage64_butterfly_3562(int16x8_t *out, int16x8_t *in){
    // Caution: Positions are different from Thesis. 
    int16x8_t butterfly[4][2] = {0};
    for(int i = 0;i<2;i++){
        // a0 + a2
        butterfly[0][i] = (vaddq_s16(in[i+4], in[i+0]));
        // a1 + a3
        butterfly[1][i] = (vaddq_s16(in[i+6], in[i+2]));
        // a0 - a2
        butterfly[2][i] = (vsubq_s16(in[i+4], in[i+0]));
        // a3 - a1
        butterfly[3][i] = (vsubq_s16(in[i+2], in[i+6]));
    }
    for(int i = 0;i<2;i++){
        // a0
        out[i  ] = barrett_mla_3562(butterfly[0][i], butterfly[3][i]);
        // a1
        out[i+2] = barrett_mls_1087(butterfly[1][i], butterfly[2][i]);
        // a2
        out[i+4] = innerProduct_3562(butterfly[3][i]);
        // a3
        out[i+6] = innerProduct_3562(butterfly[2][i]);
    }
}

void iBrunn_128_stage64(int16x8_t *out, int16x8_t *in){
    // [0-15]   (x^64-1) * (x^64+1)  ->  x^128-1
    for(int i = 0;i<4;i++){
        out[i] = (vaddq_s16(in[i], in[i+4]));
        out[i+4] = (vsubq_s16(in[i], in[i+4]));
    }
    iBruun_128_stage64_butterfly_2910(out+8, in+8);
    iBruun_128_stage64_butterfly_1087(out+16, in+16);
    iBruun_128_stage64_butterfly_3562(out+24, in+24);
}

void iBruun_128_stage32_butterfly_2910(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = barrett_fake(vaddq_s16(in[2], in[0]));
    // a1 + a3
    butterfly[1] = barrett_fake(vaddq_s16(in[3], in[1]));
    // a0 - a2
    butterfly[2] = barrett_fake(vsubq_s16(in[2], in[0]));
    // a3 - a1
    butterfly[3] = barrett_fake(vsubq_s16(in[1], in[3]));
    // a0
    out[0] = barrett_mla_2910(butterfly[0], butterfly[3]);
    // a1
    out[1] = barrett_mls_2910(butterfly[1], butterfly[2]);
    // a2
    out[2] = innerProduct_2910(butterfly[3]);
    // a3
    out[3] = innerProduct_2910(butterfly[2]);
}

void iBruun_128_stage32_butterfly_1087(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = barrett_fake(vaddq_s16(in[2], in[0]));
    // a1 + a3
    butterfly[1] = barrett_fake(vaddq_s16(in[3], in[1]));
    // a0 - a2
    butterfly[2] = barrett_fake(vsubq_s16(in[2], in[0]));
    // a3 - a1
    butterfly[3] = barrett_fake(vsubq_s16(in[1], in[3]));
    // a0
    out[0] = barrett_mla_1087(butterfly[0], butterfly[3]);
    // a1
    out[1] = barrett_mls_1029(butterfly[1], butterfly[2]);
    // a2
    out[2] = innerProduct_1087(butterfly[3]);
    // a3
    out[3] = innerProduct_1087(butterfly[2]);
}

void iBruun_128_stage32_butterfly_3562(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = barrett_fake(vaddq_s16(in[2], in[0]));
    // a1 + a3
    butterfly[1] = barrett_fake(vaddq_s16(in[3], in[1]));
    // a0 - a2
    butterfly[2] = barrett_fake(vsubq_s16(in[2], in[0]));
    // a3 - a1
    butterfly[3] = barrett_fake(vsubq_s16(in[1], in[3]));
    // a0
    out[0] = barrett_mla_3562(butterfly[0], butterfly[3]);
    // a1
    out[1] = barrett_mls_1087(butterfly[1], butterfly[2]);
    // a2
    out[2] = innerProduct_3562(butterfly[3]);
    // a3
    out[3] = innerProduct_3562(butterfly[2]);
}

void iBruun_128_stage32_butterfly_neg_1842(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = barrett_fake(vaddq_s16(in[2], in[0]));
    // a1 + a3
    butterfly[1] = barrett_fake(vaddq_s16(in[3], in[1]));
    // a0 - a2
    butterfly[2] = barrett_fake(vsubq_s16(in[2], in[0]));
    // a3 - a1
    butterfly[3] = barrett_fake(vsubq_s16(in[1], in[3]));
    // a0
    out[0] = barrett_mla_2749(butterfly[0], butterfly[3]);
    // a1
    out[1] = barrett_mla_2662(butterfly[1], butterfly[2]);
    // a2
    out[2] = innerProduct_1842(butterfly[3]);
    // a3
    out[3] = innerProduct_2749(butterfly[2]);
}

void iBruun_128_stage32_butterfly_neg_964(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = barrett_fake(vaddq_s16(in[2], in[0]));
    // a1 + a3
    butterfly[1] = barrett_fake(vaddq_s16(in[3], in[1]));
    // a0 - a2
    butterfly[2] = barrett_fake(vsubq_s16(in[2], in[0]));
    // a3 - a1
    butterfly[3] = barrett_fake(vsubq_s16(in[1], in[3]));
    
    // a0
    out[0] = barrett_mla_3627(butterfly[0], butterfly[3]);
    // a1
    out[1] = barrett_mla_2207(butterfly[1], butterfly[2]);
    // a2
    out[2] = innerProduct_964(butterfly[3]);
    // a3
    out[3] = innerProduct_3627(butterfly[2]);
}

void iBruun_128_stage32_butterfly_neg_1506(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    butterfly[0] = barrett_fake(vaddq_s16(in[2], in[0]));
    // a1 + a3
    butterfly[1] = barrett_fake(vaddq_s16(in[3], in[1]));
    // a0 - a2
    butterfly[2] = barrett_fake(vsubq_s16(in[2], in[0]));
    // a3 - a1
    butterfly[3] = barrett_fake(vsubq_s16(in[1], in[3]));
    
    // a0
    out[0] = barrett_mla_3085(butterfly[0], butterfly[3]);
    // a1
    out[1] = barrett_mla_3204(butterfly[1], butterfly[2]);
    // a2
    out[2] = innerProduct_1506(butterfly[3]);
    // a3
    out[3] = innerProduct_3085(butterfly[2]);
}

void iBruun_128_stage32_butterfly_neg_144(int16x8_t *out, int16x8_t *in){
    int16x8_t butterfly[4] = {0};
    // a0 + a2
    butterfly[0] = barrett_fake(vaddq_s16(in[2], in[0]));
    // a1 + a3
    butterfly[1] = barrett_fake(vaddq_s16(in[3], in[1]));
    // a0 - a2
    butterfly[2] = barrett_fake(vsubq_s16(in[2], in[0]));
    // a3 - a1
    butterfly[3] = barrett_fake(vsubq_s16(in[1], in[3]));
    
    // a0
    out[0] = barrett_mla_4447(butterfly[0], butterfly[3]);
    // a1
    out[1] = barrett_mla_686(butterfly[1], butterfly[2]);
    // a2
    out[2] = innerProduct_144(butterfly[3]);
    // a3
    out[3] = innerProduct_4447(butterfly[2]);
}

void iBrunn_128_stage32(int16x8_t *out, int16x8_t *in){
    // [0-7]   (x^16-1) * (x^16+1)  ->  x^64-1
    for(int i = 0;i<2;i++){
        out[i] = barrett_fake(vaddq_s16(in[i], in[i+2]));
        out[i+2] = barrett_fake(vsubq_s16(in[i], in[i+2]));
    }
    iBruun_128_stage32_butterfly_2910(out+4, in+4);
    iBruun_128_stage32_butterfly_1087(out+8, in+8);
    iBruun_128_stage32_butterfly_3562(out+12, in+12);
    iBruun_128_stage32_butterfly_neg_1842(out+16, in+16);
    iBruun_128_stage32_butterfly_neg_964(out+20, in+20);
    iBruun_128_stage32_butterfly_neg_1506(out+24, in+24);
    iBruun_128_stage32_butterfly_neg_144(out+28, in+28);
}

// 4 layers of Bruun's input FFT
void Brunn_128_In(int16x8_t *out, int16x8_t *in){
    int16x8_t tmp[32] = {0};
    Brunn_128_stage256(tmp, in);
    Brunn_128_stage128(out, tmp);
    Brunn_128_stage64(tmp, out);
    Brunn_128_stage32(out, tmp);
}

// 4 layers of Bruun's output FFT
void Brunn_128_Out(int16x8_t *out, int16x8_t *in){
    int16x8_t tmp[32] = {0};
    int16x8_t res[32] = {0};
    
    Brunn_128_modRing(tmp, in);
    iBrunn_128_stage32(res, tmp);
    iBrunn_128_stage64(tmp, res);
    iBrunn_128_stage128(res, tmp);
    iBrunn_128_stage256(out, res);
    
    // divide bt 2^4
    for(int i = 0;i<32;i++)
    {
        out[i] = innerProduct_287(out[i]);
    }
}


